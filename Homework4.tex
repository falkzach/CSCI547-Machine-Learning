\title{
	CSCI547 Machine Learning\\
	Homework 4\\
}
\author{
	Zachary Falkner\\
	Department of Computer Science\\
	University of Montana\\
}
\date{\today}
\documentclass[12pt]{article}

\usepackage{enumitem, listings, graphicx, xcolor, amsmath, tikz}

\lstset{language=Python,
	keywordstyle=\color{blue},
	basicstyle=\scriptsize\ttfamily,
	commentstyle=\ttfamily\itshape\color{gray},
	stringstyle=\ttfamily,
	showstringspaces=false,
	breaklines=true,
	frameround=ffff,
	frame=single,
	rulecolor=\color{black}
}

\usetikzlibrary {positioning}
%\usepackage {xcolor}
\definecolor {processblue}{cmyk}{0.96,0,0,0}

\begin{document}
	\maketitle
	
	\begin{flushleft}
		\section{Bayesian Networks}
		\subsection*{1A}
		$P(a,b) \neq P(a)P(b)$\\
		$0.048 \neq  0.192 * 0.048$\\
		$0.048 \neq 0.009216$\\
		\vspace{0.25cm}
		$P(a,b | c) = P(a,c)P(b,c) for c \in{0, 1}$\\
		$\frac{P(a,b,c)}{P(c)} =  P(a,c)P(b,c)$\\
		$\frac{0.096}{0.144} = 0.064 * 0.216$\\
		$0.013824 = 0.013824$\\
		$qed$\\

		\subsection*{1B}
		$P(a,b,c) = P(a)*P(c|a)*P(b|c)$\\
		$P(a,b,c) = P(a)*\frac{P(a,c)}{P(a)}*\frac{P(b,c)}{P(c)}$\\
		$0.096 = 0.192 * \frac{0.064}{0.192} * \frac{0.216}{0.144}$\\
		$0.096 = 0.096$\\
		$qed$\\
		\vspace{0.5cm}
		\begin {tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid , semithick ,
			state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
				draw,processblue , text=blue , minimum width =1 cm}]
			\node[state] (C) {$C$};
			\node[state] (B) [left =of C] {$B$};
			\node[state] (A) [left =of B] {$A$};
			\path (A) edge node {} (B);
			\path (B) edge node {} (C);	
		\end{tikzpicture}\\
		
				
		\subsection*{1C*}
		$P(A,B,C,D,E) = P(A)P(C)P(B|A,C)P(D|C)P(E|D)$\\
		
		\vspace{0.5cm}
		\begin {tikzpicture}[-latex ,auto ,node distance =4 cm and 5cm ,on grid , semithick ,
			state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
				draw,processblue , text=blue , minimum width =1 cm}]
			\node[state] (A) [] {$A$};
			\node[state] (B) [right =of A] {$B$};
			\node[state] (C) [below =of A] {$C$};
			\node[state] (D) [right =of C] {$D$};
			\node[state] (E) [right =of D] {$E$};
			
			\path (A) edge node {} (B);
			\path (C) edge node {} (B);
			\path (C) edge node {} (D);
			\path (D) edge node {} (E);
		\end{tikzpicture}\\
		\vspace{0.5cm}
		$P(A=1|E=1,C=1)$\\
		Consider the following relationships:\\
		\vspace{0.5cm}
		\begin {tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,on grid , semithick ,
			state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
				draw,processblue , text=blue , minimum width =1 cm}]
	
			\node[state] (D) [] {$D$};
			\node[state] (C) [below left =of D] {$C$};
			\node[state] (E) [below right =of D] {$E$};
			
			\path (C) edge node {} (D);
			\path (D) edge node {} (E);
		\end{tikzpicture}
		\hspace{0.5cm}
		\begin {tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,on grid , semithick ,
		state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
			draw,processblue , text=blue , minimum width =1 cm}]
		
			\node[state] (B) [] {$B$};
			\node[state] (C) [below left =of B] {$C$};
			\node[state] (A) [below right =of B] {$A$};
			
			\path (C) edge node {} (B);
			\path (A) edge node {} (B);
		\end{tikzpicture}
		\hspace{0.5cm}
		\begin {tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,on grid , semithick ,
		state/.style ={ circle ,top color =white , bottom color = processblue!20 ,
			draw,processblue , text=blue , minimum width =1 cm}]
		
			\node[state] (C) [] {$C$};
			\node[state] (B) [below left =of C] {$B$};
			\node[state] (D) [below right =of C] {$D$};
			
			\path (C) edge node {} (B);
			\path (C) edge node {} (D);
		\end{tikzpicture}\\
		\vspace{0.5cm}
		...\\
\vspace{0.5cm}
		
		
		
		
		\section{Markov Models: Gene sequence clustering}
		\subsection*{2A}
		\begin{lstlisting}
import pickle
import numpy as np 

from markov_models import FirstOrderMarkovModel

DATASET_TRAINING = "genes_training.p"

def new_sequence(class_id, models):
	return models[class_id].generate_phrase()


if __name__ == "__main__":
	training = pickle.load(open(DATASET_TRAINING, "rb"))
	
	training_data = np.array(training[0])
	training_lables = np.array(training[1])
	
	training_0 = training_data[training_lables[:] == 0]
	training_1 = training_data[training_lables[:] == 1]
	
	
	sequences_0 = training_0[0]
	seq_0 = ''.join(str(seq) for seq in sequences_0)
	
	sequences_1 = training_1[0]
	seq_1 = ''.join(str(seq) for seq in sequences_1)
	
	sequence_mm_model_0 = FirstOrderMarkovModel(seq_0)
	sequence_mm_model_0.build_transition_matrices()
	
	sequence_mm_model_1 = FirstOrderMarkovModel(seq_1)
	sequence_mm_model_1.build_transition_matrices()
	
	models = [sequence_mm_model_0, sequence_mm_model_1]
	for i in range(0,2):
		print(new_sequence(i, models))
		\end{lstlisting}
		
		Modification to markov\_models.py\\
		\begin{lstlisting}
def generate_phrase(self, length=20):
	phrase = ''
	for i in range(0,length):
		w_minus_1 = np.random.choice(list(self.transitions[0].keys()),replace=True,p=list(self.transitions[0].values()))
		phrase += w_minus_1
	return phrase
		\end{lstlisting}
		
		\begin{lstlisting}
>python 2a.py
TTTCCATTGTCGGATAAATT
AACCGGTGAGACATGCAGCA

		\end{lstlisting}
		
		\subsection*{2B}
		
		\begin{lstlisting}
import pickle
import numpy as np 

from markov_models import FirstOrderMarkovModel

DATASET_TRAINING = "genes_training.p"
DATASET_TEST = "genes_test.p"

if __name__ == "__main__":
	training = pickle.load(open(DATASET_TRAINING, "rb"))
	test = pickle.load(open(DATASET_TEST, "rb"))
	
	training_data = np.array(training[0])
	training_lables = np.array(training[1])
	test_data = np.array(test[0])
	test_labels = np.array(test[1])
	
	sequences_0 = training_data[training_lables == 0]
	sequences_1 = training_data[training_lables == 1]
	
	seq_0 = ''.join(str(seq) for seq in sequences_0)
	seq_1 = ''.join(str(seq) for seq in sequences_1)
	
	sequence_mm_model_0 = FirstOrderMarkovModel(seq_0)
	sequence_mm_model_0.build_transition_matrices()
	
	sequence_mm_model_1 = FirstOrderMarkovModel(seq_1)
	sequence_mm_model_1.build_transition_matrices()
	
	predictions = []
	
	for sequence in test_data:
		sequence = ' '.join(sequence)
		scores = []
		scores.append(sequence_mm_model_0.compute_log_likelihood(sequence))
		scores.append(sequence_mm_model_1.compute_log_likelihood(sequence))
		predictions.append(np.argmax(scores))
	
	total = test_labels.size
	correct = np.sum(predictions == test_labels)
	accuracy = correct/total
	print("Accuracy: {}".format(accuracy))

		\end{lstlisting}
		
		\begin{lstlisting}
>python 2b.py
Accuracy: 0.985
		\end{lstlisting}
		
		\subsection*{2C*}
		
		\begin{lstlisting}
import pickle
import numpy as np 

from markov_models import NaiveBayesModel

DATASET_TRAINING = "genes_training.p"
DATASET_TEST = "genes_test.p"

if __name__ == "__main__":
	training = pickle.load(open(DATASET_TRAINING, "rb"))
	test = pickle.load(open(DATASET_TEST, "rb"))
	
	training_data = np.array(training[0])
	training_lables = np.array(training[1])
	test_data = np.array(test[0])
	test_labels = np.array(test[1])
	
	sequences_0 = training_data[training_lables == 0]
	sequences_1 = training_data[training_lables == 1]
	
	seq_0 = ''.join(str(seq) for seq in sequences_0)
	seq_1 = ''.join(str(seq) for seq in sequences_1)
	
	sequence_nb_model_0 = NaiveBayesModel(seq_0)
	sequence_nb_model_0.build_transition_matrices()
	
	sequence_nb_model_1 = NaiveBayesModel(seq_1)
	sequence_nb_model_1.build_transition_matrices()
	
	predictions = []
	
	for sequence in test_data:
		sequence = ' '.join(sequence)
		scores = []
		scores.append(sequence_nb_model_0.compute_log_likelihood(sequence))
		scores.append(sequence_nb_model_1.compute_log_likelihood(sequence))
		predictions.append(np.argmax(scores))
	
	total = test_labels.size
	correct = np.sum(predictions == test_labels)
	accuracy = correct/total
	print("Accuracy: {}".format(accuracy))

		\end{lstlisting}
		
		\begin{lstlisting}
>python 2c.py
Accuracy: 0.899
		\end{lstlisting}
		
		Simply put, naive Bayes' is naive It takes no consideration as to the ordering of the nucleobases, only their frequency. While yes this produces a random string made up of ATGC, it does not necessarily mean that it will look anything like a real snippet of genetic code. The Markov model takes into account the likelihood of characters following each other which is why it produces a significantly better result when used as the model.\\

	\end{flushleft}
\end{document}